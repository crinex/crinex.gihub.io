<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->
    <meta property="og:title"
        content="OSA-Net: An Efficient Convolutional Neural Network for OSA Diagnosis Screening Tool" />
    <!-- <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" /> -->
    <meta property="og:url" content="https://crinex.github.io/pages/OSA-net/" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <!-- <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" /> -->


    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->


    <title>OSA-Net</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/koala.png">-->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/index.js"></script>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.1.1/gradio.js"></script>
</head>

<body>


    <section class="hero banner">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title"></i>OSA-Net<br />
                            :An Efficient Convolutional Neural Network for OSA Diagnosis Screening Tool</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                                <a href="https://aunal.org/people/jypark" target="_blank">â€ June-Young Park</a><sup style="color:#aa0404;">1 </sup>,</span>
                            <span class="author-block">
                                <a href="https://www.dkuh.co.kr/html_2016/03/01_02.php?idx=456150&url=%2Fhtml_2016%2F03%2F01.php&drword=%EC%8B%A0%ED%98%9C%EB%A6%BC" target="_blank">Hye-Rim Shin</a><sup style="color:#1b5dd7;">2</sup>,</span>
                            <span class="author-block">
                                <a href="https://aunal.org/people/tjkim" target="_blank">Tae-Joon Kim</a><sup style="color:#ecaa2e;">3</sup></span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup style="color:#aa0404;">1</sup>Graduate School of Medicine Ajou University,</span>
                            <span class="author-block"><sup style="color:#1b5dd7;">2</sup>Dankook University Hospital,</span>
                            <span class="author-block"><sup style="color:#ecaa2e;">3</sup>Ajou University School of Medicine</span>
                        </div>
                        â€ {crinexk@gmail.com}
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://ieeexplore.ieee.org/document/10320072" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>IPTA 2023</span>
                    
                                    </a>
                                </span>


                                <!-- <span class="link-block">
                                    <a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/pixart" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">ðŸ§¨</span>
                                        <span>Diffusers</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">ðŸ¤—</span>
                                        <span>HF Demo</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://openxlab.org.cn/apps/detail/PixArt-alpha/PixArt-alpha" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">X</span>
                                        <span>OpenXLab Demo</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2310.00426" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://discord.gg/hWT7caau" target="_blank"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-discord"></i>
                                        </span>
                                        <span>Discord</span>
                                    </a>
                                </span> -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Image carousel -->
    <!-- End image carousel -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Obstructive sleep apnea (OSA) has a high prevalence worldwide, particularly among adults and the population with comorbidities. 
                            Adult men have a higher incidence rate and approximately 15-30% prevalence, and OSA is associated with other diseases such as obesity, hypertension, and diabetes. 
                            Polysomnography is essential for diagnosing OSA, but discomfort and overnight testing are practical difficulties. 
                            Therefore, several questionnaires, such as the STOP-BANG and Berlin questionnaire, have been developed for initial screening of OSA, although their use is limited due to their low accuracy. 
                            One of the pathophysiology of OSA is related to craniofacial anatomy, and several previous studies have investigated facial anatomy using 2D or 3D photographs with a small number of patients, 
                            but practical application for screening and diagnosing OSA has not been attempted yet. 
                            Therefore, our research aims to screen OSA and stratify its severity with feasible 2D photographs and a big dataset using a novel deep learning algorithm. 
                            We developed a new CNN-based algorithm called OSA-Net, which diagnoses OSA patients using information on facial anatomical abnormalities by ready-to-use photographs. 
                            Facial pictures of 900 patients who underwent polysomnography were used as a dataset for model training. 
                            The results of our algorithm showed a high diagnostic accuracy over 85% for classifying OSA severity into normal, mild, moderate, and severe degrees. 
                            Furthermore, we used Grad-CAM to verify the accuracy of our model, and the results showed that our model accurately recognizes facial contours. 
                            This study demonstrates the potential of using artificial intelligence with image big data as a new OSA screening tool.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->
    
    <!-- Start demo page -->
    <!-- <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-centered has-text-centered">
                <h2 class="title is-3">Online Demo</h2>
                <gradio-app src="https://pixart-alpha-pixart-alpha.hf.space" style="display: flex; justify-content: center;"></gradio-app>
            </div>
        </div>
    </section> -->
    <!-- End demo page -->


    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <h2 class="title is-3">Datasets & Preprocessing</h2>
                <p>
                    To train and evaluate our proposed OSA-Net model,
                    we used facial images of patients who underwent PSG tests
                    at the Neurology Department of Cheonan Dankook
                    University Hospital from 2012 to 2022. 
                    The data were obtained from adults aged 19 or older and Korean nationals.
                    PSG technicians collected facial photos using the following
                    protocol and equipment. Equipment: Samsung digital camera
                    WB350F. Photography: Facial photos were taken once each
                    from the front and left at 90-degree angles.
                    OSA patients typically have a thick
                    neck and jaw, which is caused by an increase in the amount
                    of muscle and adipose tissue that repetitively obstructs the
                    upper airway. This phenomenon may be related to obesity
                    and may interact with congenital anatomical features of the
                    face, further exacerbating the repetitive obstruction of the
                    upper airway.
                    Unnecessary
                    parts of the images, such as the background, are removed.
                    Using MediaPipe, an open-source face recognition library in
                    Python, we are able to acquire four coordinates (left top, right
                    top, left bottom, right bottom) that enclose the face area.
                    Based on this coordinate information, we crop the original
                    image to obtain only the side face. Additionally, taking
                    into consideration the conditions of the hardware, all images
                    have been resized to (256*256).
                </p>
                <br>
                <img loading="lazy" src="static/images/prepro-pipeline.png" />
            </div>
        </div>
    </section>
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <h2 class="title is-3">OSA-Net Architecture</h2>
                <p>The structure of the model we propose is shown in the figure below.</p>
                <img loading='lazy' src='static/images/net-archi.png' style="width: 100%;" />
                <p>
                    OSA-Net is composed of six main stages, with key blocks
                    and layers including the Stem Block, SA Block, global
                    average pooling (GAP), and fully connected layer. CNNs are
                    generally divided into feature extraction and classification
                    areas, and in OSA-Net, Stages 1-5 serve as the feature
                    extraction area while Stage 6 serves as the classification area
                    for determining the class of OSA.
                    SA Block is a core block of our proposed deep
                    learning model based on depthwise-separable convolutional
                    layer. It is composed of 2D-convolutional layer, batch
                    normalization, activation function, depthwise-separable
                    convolutional layer, and SE Block, and is present in OSANet's
                    Stage 2-5 (see Table 1, Stage 2-5). Although all SA
                    Blocks have the same structure, the main parameters of the
                    2D-convolutional layer are different for each one. The filter
                    size of the SA Block's 2D-convolutional layer is (1*1). When
                    the filter size of the convolutional layer is (1*1), the size of
                    the input feature map and the output feature map remains the
                    same, preserving spatial information between them. This is
                    effective in reducing computational complexity and the
                    number of parameters in the model, thus positively impacting
                    the model's training speed and generalization ability.
                </p>
                <br>
                <img loading="lazy" src="static/images/sa-structure.png" alt="human" style="width: 100%; max-width: none;" />
                <!-- <img loading="lazy" src="static/images/controlnet/controlnet_iclr.svg" alt="iclr" /> -->
            </div>
        </div>
    </section>
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <h2 class="title is-3">Quantitative comparison with related works</h2>
                <h2>The table below shown the results of a quantitative comparison with previous related works.
                </h2>
                <img loading="lazy" src="static/images/osanet-result-table.png" alt="comp" style="width: 100%; max-width: none;" />
            </div>
        </div>
    </section>
    
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <h2 class="title is-3">Model interpretation using Grad-CAM</h2>
                <h2>Grad-CAM is used to interpret the results of a
                    model. Grad-CAM is one of the methodologies for visually
                    interpreting deep learning models.
                </h2>
                <img loading="lazy" src="static/images/osa_cam.png" alt="comp" style="width: 100%; max-width: none;" />
            </div>
        </div>
    </section>


    <!--BibTex citation -->

    <!--End BibTex citation -->



    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>
